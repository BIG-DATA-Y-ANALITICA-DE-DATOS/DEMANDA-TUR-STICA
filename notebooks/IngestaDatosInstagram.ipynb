{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fa7a41-3f90-44a2-88ae-932fe434ab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando e iniciando la sesión de Spark...\n",
      "Sesión de Spark iniciada.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTACIONES E INICIO DE SESIÓN DE SPARK\n",
    "# ==============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, to_date, count, when\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# Detiene cualquier sesión de Spark existente para asegurar una configuración limpia\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# --- Configuración de Conexión y Spark ---\n",
    "path_al_jar = \"/home/user/BigData_UPAO/postgresql-42.7.3.jar\"\n",
    "db_host = \"aws-1-us-east-2.pooler.supabase.com\"\n",
    "db_password = \"2EOJG5w9K48yY4Qq\"\n",
    "db_user = \"postgres.nnoedhfsidxvosrvlrqb\"\n",
    "db_name = \"postgres\"\n",
    "\n",
    "print(\"Configurando e iniciando la sesión de Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Supabase_To_HDFS\") \\\n",
    "    .config(\"spark.jars\", path_al_jar) \\\n",
    "    .config(\"spark.driver.extraClassPath\", path_al_jar) \\\n",
    "    .getOrCreate()\n",
    "print(\"Sesión de Spark iniciada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d64684-6d64-4627-8d50-471b58a73642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Supabase y cargando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados desde Supabase. Se leyeron 100 registros.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. LECTURA DE DATOS DESDE SUPABASE\n",
    "# ==============================================================================\n",
    "jdbc_url = f\"jdbc:postgresql://{db_host}:6543/{db_name}\"\n",
    "connection_properties = {\"user\": db_user, \"password\": db_password, \"driver\": \"org.postgresql.Driver\"}\n",
    "query = \"(SELECT * FROM public.instagram_posts) AS posts_completos\"\n",
    "\n",
    "print(\"Conectando a Supabase y cargando datos...\")\n",
    "df_from_supabase = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
    "print(f\"Datos cargados desde Supabase. Se leyeron {df_from_supabase.count()} registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3c6715-310b-4616-85cc-89b5d6497e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, post_id: string, username: string, caption: string, location: string, hashtags: string, mentions: string, engagement: string, created_at: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0dbdf7-42ca-491c-b927-e435705a2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. DEFINICIÓN DE LÓGICA DE PROCESAMIENTO (UDFs)\n",
    "# ==============================================================================\n",
    "TRANSPORT_KEYWORDS = {'aéreo': ['vuelo', 'volamos', 'avión', 'latam', 'sky airline', 'jetsmart', 'star peru', 'avioneta'],'terrestre': ['bus', 'carro', 'auto', 'camioneta', 'cruz del sur', 'movil tours', 'en carretera', 'van'],'tren': ['tren', 'perurail', 'inca rail', 'andean explorer'],'marítimo': ['bote', 'lancha', 'barco', 'navegando']}\n",
    "ACCOMMODATION_KEYWORDS = {'hotel': ['hotel', 'resort', 'lodge', 'boutique'],'hostal': ['hostal', 'hostel', 'hospedaje'],'otro': ['airbnb', 'casa', 'bungalow', 'cabaña', 'campamento', 'acampamos', 'camping']}\n",
    "PERUVIAN_DESTINATIONS = ['cusco', 'machu picchu', 'arequipa', 'lima', 'iquitos', 'mancora', 'paracas', 'huaraz','nazca', 'puno', 'lago titicaca', 'trujillo', 'chiclayo', 'tarapoto', 'ayacucho','cajamarca', 'chachapoyas', 'kuelap', 'gocta', 'vinicunca', 'montaña de 7 colores','huacachina', 'ica', 'ollantaytambo', 'valle sagrado', 'urubamba', 'colca', 'salkantay','choquequirao', 'tambopata', 'pacaya samiria', 'lunahuaná']\n",
    "\n",
    "def extract_entity(text, keywords_dict):\n",
    "    if text is None: return None\n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in keywords_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text_lower):\n",
    "                return category\n",
    "    return None\n",
    "\n",
    "def extract_duration(text):\n",
    "    if text is None: return None\n",
    "    text_lower = text.lower()\n",
    "    match = re.search(r'(\\d+)\\s*(días|noches)', text_lower)\n",
    "    if match: return f\"{match.group(1)} días/noches\"\n",
    "    if 'una semana' in text_lower or 'semana entera' in text_lower: return \"1 semana\"\n",
    "    if 'dos semanas' in text_lower: return \"2 semanas\"\n",
    "    if 'fin de semana' in text_lower or 'finde' in text_lower: return \"fin de semana\"\n",
    "    if 'full day' in text_lower or 'un día' in text_lower or 'medio día' in text_lower: return \"1 día (Full Day)\"\n",
    "    return None\n",
    "\n",
    "def extract_location_spark(location_struct, caption):\n",
    "    if location_struct and hasattr(location_struct, 'name') and location_struct.name:\n",
    "        location_name = location_struct.name.split(',')[0].strip()\n",
    "        if location_name.lower() not in ['peru', 'cusco', 'lima'] or len(location_struct.name.split(',')) == 1:\n",
    "            return location_name\n",
    "    if caption:\n",
    "        text_lower = caption.lower()\n",
    "        for dest in PERUVIAN_DESTINATIONS:\n",
    "            if dest in text_lower:\n",
    "                return dest.title()\n",
    "    if location_struct and hasattr(location_struct, 'name') and location_struct.name:\n",
    "        return location_struct.name.split(',')[0].strip()\n",
    "    return None\n",
    "\n",
    "extract_transport_udf = udf(lambda caption: extract_entity(caption, TRANSPORT_KEYWORDS), StringType())\n",
    "extract_accommodation_udf = udf(lambda caption: extract_entity(caption, ACCOMMODATION_KEYWORDS), StringType())\n",
    "extract_duration_udf = udf(extract_duration, StringType())\n",
    "extract_location_udf = udf(extract_location_spark, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10b28d2-3abe-428a-bb3d-824c9476f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando transformaciones al DataFrame...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. APLICACIÓN DE TRANSFORMACIONES\n",
    "# ==============================================================================\n",
    "print(\"Aplicando transformaciones al DataFrame...\")\n",
    "\n",
    "df_processed = df_from_supabase.withColumn(\"lugar_viaje\", extract_location_udf(col(\"location\"), col(\"caption\"))) \\\n",
    "                           .withColumn(\"fecha\", to_date(col(\"created_at\"))) \\\n",
    "                           .withColumn(\"medio_transporte\", extract_transport_udf(col(\"caption\"))) \\\n",
    "                           .withColumn(\"duracion\", extract_duration_udf(col(\"caption\"))) \\\n",
    "                           .withColumn(\"alojamiento\", extract_accommodation_udf(col(\"caption\")))\n",
    "\n",
    "df_final = df_processed.select(\"lugar_viaje\", \"fecha\", \"medio_transporte\", \"duracion\", \"alojamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0cce30-d9d7-4581-9634-a704356b45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vista Previa del Dataset Procesado ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------------+----------------+-----------+\n",
      "|lugar_viaje|fecha     |medio_transporte|duracion        |alojamiento|\n",
      "+-----------+----------+----------------+----------------+-----------+\n",
      "|Cusco      |2025-05-22|aéreo           |3 días/noches   |hotel      |\n",
      "|Iquitos    |2025-08-10|aéreo           |1 semana        |hotel      |\n",
      "|Lima       |2025-02-15|terrestre       |5 días/noches   |otro       |\n",
      "|Cusco      |2025-09-05|tren            |4 días/noches   |NULL       |\n",
      "|Lima       |2025-03-28|NULL            |fin de semana   |otro       |\n",
      "|Arequipa   |2025-07-19|aéreo           |3 días/noches   |hotel      |\n",
      "|Lima       |2025-04-12|terrestre       |2 días/noches   |hostal     |\n",
      "|Chachapoyas|2025-06-30|aéreo           |1 semana        |hotel      |\n",
      "|Nazca      |2025-10-14|aéreo           |1 día (Full Day)|NULL       |\n",
      "|Arequipa   |2025-08-25|terrestre       |3 días/noches   |NULL       |\n",
      "|Cusco      |2025-07-27|aéreo           |1 semana        |otro       |\n",
      "|Lima       |2025-01-20|terrestre       |fin de semana   |hotel      |\n",
      "|Cusco      |2025-09-18|NULL            |1 día (Full Day)|NULL       |\n",
      "|Lima       |2025-05-10|terrestre       |4 días/noches   |hostal     |\n",
      "|Chiclayo   |2025-04-02|aéreo           |3 días/noches   |hotel      |\n",
      "+-----------+----------+----------------+----------------+-----------+\n",
      "only showing top 15 rows\n",
      "\n",
      "--- Resumen de Valores Nulos por Columna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------------+--------+-----------+\n",
      "|lugar_viaje|fecha|medio_transporte|duracion|alojamiento|\n",
      "+-----------+-----+----------------+--------+-----------+\n",
      "|         15|    0|              51|       5|         67|\n",
      "+-----------+-----+----------------+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. MOSTRAR RESULTADOS Y GUARDAR EN HDFS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Vista Previa del Dataset Procesado ---\")\n",
    "df_final.show(15, truncate=False)\n",
    "\n",
    "print(\"\\n--- Resumen de Valores Nulos por Columna ---\")\n",
    "df_final.select([count(when(col(c).isNull(), c)).alias(c) for c in df_final.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2243d78a-0819-4296-ab65-6d61367939ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados como CSV en HDFS en la ruta: hdfs://localhost:9000/data/processed/instagram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "hdfs_path_csv = \"hdfs://localhost:9000/data/processed/instagram\"\n",
    "df_final.write.mode(\"overwrite\").option(\"header\", \"true\").csv(hdfs_path_csv)\n",
    "\n",
    "print(f\"Datos guardados como CSV en HDFS en la ruta: {hdfs_path_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "942c0f88-56b2-4734-8077-8398bd1f6ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado. Sesión de Spark detenida.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. DETENER LA SESIÓN DE SPARK\n",
    "# ==============================================================================\n",
    "spark.stop()\n",
    "print(\"Proceso completado. Sesión de Spark detenida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83852264-3700-4095-a3dd-8ffdabbe1832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Big Data)",
   "language": "python",
   "name": "bigdata_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

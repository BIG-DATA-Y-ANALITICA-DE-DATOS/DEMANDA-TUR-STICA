{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb90dee",
   "metadata": {},
   "source": [
    "# Probando conexion con la base de datos PostgreSQL local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26aa8b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./bigdata_env/lib/python3.11/site-packages (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2668ed9-5388-4a81-bc5d-5ffc15b7dc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc4cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Current Time: (datetime.datetime(2025, 11, 3, 3, 35, 16, 182230, tzinfo=datetime.timezone.utc),)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch variables\n",
    "USER = os.getenv(\"user\")\n",
    "PASSWORD = os.getenv(\"password\")\n",
    "HOST = os.getenv(\"host\")\n",
    "PORT = os.getenv(\"port\")\n",
    "DBNAME = os.getenv(\"dbname\")\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        dbname=DBNAME\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "    # Create a cursor to execute SQL queries\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Example query\n",
    "    cursor.execute(\"SELECT NOW();\")\n",
    "    result = cursor.fetchone()\n",
    "    print(\"Current Time:\", result)\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    #cursor.close()\n",
    "    #connection.close()\n",
    "    #print(\"Connection closed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e7828",
   "metadata": {},
   "source": [
    "## Listando tablas disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6231092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas encontradas:\n",
      "n8n_chat_histories\n",
      "airbnb_listings_final\n",
      "geography_columns\n",
      "geometry_columns\n",
      "spatial_ref_sys\n",
      "scraped_data\n",
      "tweets_results\n",
      "google_search\n",
      "instagram_posts\n",
      "trivago_hoteles\n",
      "facebook_posts\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public'\n",
    "\"\"\")\n",
    "\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tablas encontradas:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b4a55",
   "metadata": {},
   "source": [
    "## Listando campos de la tabla instagram_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5467f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en instagram_posts:\n",
      "- created_at (timestamp with time zone)\n",
      "- mentions (jsonb)\n",
      "- engagement (jsonb)\n",
      "- id (uuid)\n",
      "- location (jsonb)\n",
      "- hashtags (jsonb)\n",
      "- post_id (text)\n",
      "- username (text)\n",
      "- caption (text)\n"
     ]
    }
   ],
   "source": [
    "table_name = 'instagram_posts'  \n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT column_name, data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_name = %s\n",
    "\"\"\", (table_name,))\n",
    "\n",
    "columns = cursor.fetchall()\n",
    "print(f\"Columnas en {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"- {column[0]} ({column[1]})\")\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808f4b9",
   "metadata": {},
   "source": [
    "# Seleccionamos la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125a619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7043/4114259078.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id post_id           username  \\\n",
      "0  54e656fd-04cd-4c84-aa05-6f5dc977d0a0  IG_001  aventurero_andino   \n",
      "1  d85d4f85-f5e5-4c1b-9cb6-d7344d44c955  IG_002  exploradora_selva   \n",
      "2  308dc8dc-26c2-473f-b3df-17566b6cdb18  IG_003   playas_del_norte   \n",
      "3  3c650525-c5be-4478-9c95-e2f334216a6e  IG_004     trekking_lover   \n",
      "4  efaebd80-e219-4d74-b852-bffc8d89abe1  IG_005        foodie_lima   \n",
      "\n",
      "                                             caption  \\\n",
      "0  ¡Increíble fin de semana en Cusco! Volamos con...   \n",
      "1  Una semana completa en Iquitos, explorando el ...   \n",
      "2  Disfrutando del sol en Máncora por 5 días. Tom...   \n",
      "3  Ruta del Salkantay a Machu Picchu. Fueron 4 dí...   \n",
      "4  Fin de semana gastronómico en Lima. Nos movimo...   \n",
      "\n",
      "                       location                                 hashtags  \\\n",
      "0       {'name': 'Cusco, Peru'}  [Cusco, PeruTravel, MachuPicchu, Viaje]   \n",
      "1   {'name': 'Iquitos, Loreto'}        [Iquitos, Amazonas, SelvaPeruana]   \n",
      "2    {'name': 'Mancora, Piura'}            [Mancora, PlayasPeru, Verano]   \n",
      "3   {'name': 'Salkantay Trail'}   [Salkantay, Trekking, Cusco, Aventura]   \n",
      "4  {'name': 'Miraflores, Lima'}       [Lima, GastronomiaPeruana, Foodie]   \n",
      "\n",
      "          mentions                        engagement                created_at  \n",
      "0     [@latamperu]   {'likes': 1500, 'comments': 50} 2025-05-22 14:00:00+00:00  \n",
      "1               []   {'likes': 2200, 'comments': 85} 2025-08-10 11:30:00+00:00  \n",
      "2  [@cruzdelsurpe]   {'likes': 1800, 'comments': 70} 2025-02-15 18:45:00+00:00  \n",
      "3      [@perurail]  {'likes': 3100, 'comments': 120} 2025-09-05 20:10:00+00:00  \n",
      "4               []    {'likes': 950, 'comments': 40} 2025-03-28 22:00:00+00:00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch variables\n",
    "USER = os.getenv(\"user\")\n",
    "PASSWORD = os.getenv(\"password\")\n",
    "HOST = os.getenv(\"host\")\n",
    "PORT = os.getenv(\"port\")\n",
    "DBNAME = os.getenv(\"dbname\")\n",
    "\n",
    "# Conexión a tu base de datos\n",
    "connection = psycopg2.connect(\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        dbname=DBNAME\n",
    "    )\n",
    "\n",
    "table_name = 'instagram_posts'\n",
    "query = f\"SELECT * FROM {table_name};\"\n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "df.to_csv(\"instagram_posts.csv\", index=False, encoding='utf-8', quoting=1)\n",
    "print(df.head())\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "733025dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype              \n",
      "---  ------      --------------  -----              \n",
      " 0   id          100 non-null    object             \n",
      " 1   post_id     100 non-null    object             \n",
      " 2   username    100 non-null    object             \n",
      " 3   caption     100 non-null    object             \n",
      " 4   location    100 non-null    object             \n",
      " 5   hashtags    100 non-null    object             \n",
      " 6   mentions    100 non-null    object             \n",
      " 7   engagement  100 non-null    object             \n",
      " 8   created_at  100 non-null    datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), object(8)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6a8d6",
   "metadata": {},
   "source": [
    "# Almacenamos los datos en HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3868f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe4ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado a HDFS: raw/instagram/instagram_posts.csv\n"
     ]
    }
   ],
   "source": [
    "hdfs_path = \"raw/instagram/instagram_posts.csv\"\n",
    "\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", \"instagram_posts.csv\", hdfs_path], check=True)\n",
    "\n",
    "print(\"Archivo cargado a HDFS:\", hdfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59233e",
   "metadata": {},
   "source": [
    "# Procesamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9fa7a41-3f90-44a2-88ae-932fe434ab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando e iniciando la sesión de Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/02 22:38:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesión de Spark iniciada.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTACIONES E INICIO DE SESIÓN DE SPARK\n",
    "# ==============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, to_date, count, when\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType, TimestampType\n",
    "import re\n",
    "\n",
    "# Detiene cualquier sesión de Spark existente para asegurar una configuración limpia\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(\"Configurando e iniciando la sesión de Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Instagram_HDFS\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Sesión de Spark iniciada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55d64684-6d64-4627-8d50-471b58a73642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos desde HDFS...\n",
      "Datos cargados desde HDFS. Se leyeron 100 registros.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. LECTURA DE DATOS DESDE HDFS\n",
    "# ==============================================================================\n",
    "\n",
    "# Definir esquema basado en las columnas reales de Instagram\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"post_id\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"caption\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"hashtags\", StringType(), True),\n",
    "    StructField(\"mentions\", StringType(), True),\n",
    "    StructField(\"engagement\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "print(\"Leyendo datos desde HDFS...\")\n",
    "df_from_supabase = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"hdfs://localhost:9000/user/user/raw/instagram/instagram_posts.csv\")\n",
    "\n",
    "print(f\"Datos cargados desde HDFS. Se leyeron {df_from_supabase.count()} registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed3c6715-310b-4616-85cc-89b5d6497e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, post_id: string, username: string, caption: string, location: string, hashtags: string, mentions: string, engagement: string, created_at: timestamp]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e0dbdf7-42ca-491c-b927-e435705a2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. DEFINICIÓN DE LÓGICA DE PROCESAMIENTO (UDFs)\n",
    "# ==============================================================================\n",
    "TRANSPORT_KEYWORDS = {'aéreo': ['vuelo', 'volamos', 'avión', 'latam', 'sky airline', 'jetsmart', 'star peru', 'avioneta'],'terrestre': ['bus', 'carro', 'auto', 'camioneta', 'cruz del sur', 'movil tours', 'en carretera', 'van'],'tren': ['tren', 'perurail', 'inca rail', 'andean explorer'],'marítimo': ['bote', 'lancha', 'barco', 'navegando']}\n",
    "ACCOMMODATION_KEYWORDS = {'hotel': ['hotel', 'resort', 'lodge', 'boutique'],'hostal': ['hostal', 'hostel', 'hospedaje'],'otro': ['airbnb', 'casa', 'bungalow', 'cabaña', 'campamento', 'acampamos', 'camping']}\n",
    "PERUVIAN_DESTINATIONS = ['cusco', 'machu picchu', 'arequipa', 'lima', 'iquitos', 'mancora', 'paracas', 'huaraz','nazca', 'puno', 'lago titicaca', 'trujillo', 'chiclayo', 'tarapoto', 'ayacucho','cajamarca', 'chachapoyas', 'kuelap', 'gocta', 'vinicunca', 'montaña de 7 colores','huacachina', 'ica', 'ollantaytambo', 'valle sagrado', 'urubamba', 'colca', 'salkantay','choquequirao', 'tambopata', 'pacaya samiria', 'lunahuaná']\n",
    "\n",
    "def extract_entity(text, keywords_dict):\n",
    "    if text is None: return None\n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in keywords_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text_lower):\n",
    "                return category\n",
    "    return None\n",
    "\n",
    "def extract_duration(text):\n",
    "    if text is None: return None\n",
    "    text_lower = text.lower()\n",
    "    match = re.search(r'(\\d+)\\s*(días|noches)', text_lower)\n",
    "    if match: return f\"{match.group(1)} días/noches\"\n",
    "    if 'una semana' in text_lower or 'semana entera' in text_lower: return \"1 semana\"\n",
    "    if 'dos semanas' in text_lower: return \"2 semanas\"\n",
    "    if 'fin de semana' in text_lower or 'finde' in text_lower: return \"fin de semana\"\n",
    "    if 'full day' in text_lower or 'un día' in text_lower or 'medio día' in text_lower: return \"1 día (Full Day)\"\n",
    "    return None\n",
    "\n",
    "def extract_location_spark(location_struct, caption):\n",
    "    if location_struct and hasattr(location_struct, 'name') and location_struct.name:\n",
    "        location_name = location_struct.name.split(',')[0].strip()\n",
    "        if location_name.lower() not in ['peru', 'cusco', 'lima'] or len(location_struct.name.split(',')) == 1:\n",
    "            return location_name\n",
    "    if caption:\n",
    "        text_lower = caption.lower()\n",
    "        for dest in PERUVIAN_DESTINATIONS:\n",
    "            if dest in text_lower:\n",
    "                return dest.title()\n",
    "    if location_struct and hasattr(location_struct, 'name') and location_struct.name:\n",
    "        return location_struct.name.split(',')[0].strip()\n",
    "    return None\n",
    "\n",
    "extract_transport_udf = udf(lambda caption: extract_entity(caption, TRANSPORT_KEYWORDS), StringType())\n",
    "extract_accommodation_udf = udf(lambda caption: extract_entity(caption, ACCOMMODATION_KEYWORDS), StringType())\n",
    "extract_duration_udf = udf(extract_duration, StringType())\n",
    "extract_location_udf = udf(extract_location_spark, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d10b28d2-3abe-428a-bb3d-824c9476f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando transformaciones al DataFrame...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. APLICACIÓN DE TRANSFORMACIONES\n",
    "# ==============================================================================\n",
    "print(\"Aplicando transformaciones al DataFrame...\")\n",
    "\n",
    "df_processed = df_from_supabase.withColumn(\"lugar_viaje\", extract_location_udf(col(\"location\"), col(\"caption\"))) \\\n",
    "                           .withColumn(\"fecha\", to_date(col(\"created_at\"))) \\\n",
    "                           .withColumn(\"medio_transporte\", extract_transport_udf(col(\"caption\"))) \\\n",
    "                           .withColumn(\"duracion\", extract_duration_udf(col(\"caption\"))) \\\n",
    "                           .withColumn(\"alojamiento\", extract_accommodation_udf(col(\"caption\")))\n",
    "\n",
    "df_final = df_processed.select(\"lugar_viaje\", \"fecha\", \"medio_transporte\", \"duracion\", \"alojamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef0cce30-d9d7-4581-9634-a704356b45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vista Previa del Dataset Procesado ---\n",
      "+------------+----------+----------------+----------------+-----------+\n",
      "|lugar_viaje |fecha     |medio_transporte|duracion        |alojamiento|\n",
      "+------------+----------+----------------+----------------+-----------+\n",
      "|Cusco       |2025-05-22|aéreo           |3 días/noches   |hotel      |\n",
      "|Iquitos     |2025-08-10|aéreo           |1 semana        |hotel      |\n",
      "|Lima        |2025-02-15|terrestre       |5 días/noches   |otro       |\n",
      "|Cusco       |2025-09-05|tren            |4 días/noches   |NULL       |\n",
      "|Lima        |2025-03-28|NULL            |fin de semana   |otro       |\n",
      "|Arequipa    |2025-07-19|aéreo           |3 días/noches   |hotel      |\n",
      "|Lima        |2025-04-12|terrestre       |2 días/noches   |hostal     |\n",
      "|Arequipa    |2025-08-25|terrestre       |3 días/noches   |NULL       |\n",
      "|Cusco       |2025-07-27|aéreo           |1 semana        |otro       |\n",
      "|Cusco       |2025-09-18|NULL            |1 día (Full Day)|NULL       |\n",
      "|Lima        |2025-05-10|terrestre       |4 días/noches   |hostal     |\n",
      "|Chiclayo    |2025-04-02|aéreo           |3 días/noches   |hotel      |\n",
      "|Iquitos     |2025-11-05|aéreo           |5 días/noches   |hotel      |\n",
      "|NULL        |2025-02-28|terrestre       |1 semana        |NULL       |\n",
      "|Machu Picchu|2025-10-01|tren            |2 días/noches   |hostal     |\n",
      "+------------+----------+----------------+----------------+-----------+\n",
      "only showing top 15 rows\n",
      "\n",
      "--- Resumen de Valores Nulos por Columna ---\n",
      "+-----------+-----+----------------+--------+-----------+\n",
      "|lugar_viaje|fecha|medio_transporte|duracion|alojamiento|\n",
      "+-----------+-----+----------------+--------+-----------+\n",
      "|         15|    0|              51|       5|         67|\n",
      "+-----------+-----+----------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. MOSTRAR RESULTADOS Y GUARDAR EN HDFS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Vista Previa del Dataset Procesado ---\")\n",
    "df_final.show(15, truncate=False)\n",
    "\n",
    "print(\"\\n--- Resumen de Valores Nulos por Columna ---\")\n",
    "df_final.select([count(when(col(c).isNull(), c)).alias(c) for c in df_final.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2243d78a-0819-4296-ab65-6d61367939ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados como CSV en HDFS en la ruta: hdfs://localhost:9000/user/user/processed/instagram/instagram_posts_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Guardamos los datos procesados en HDFS\n",
    "hdfs_path_csv = \"hdfs://localhost:9000/user/user/processed/instagram/instagram_posts_clean.csv\"\n",
    "\n",
    "df_final.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(hdfs_path_csv)\n",
    "\n",
    "print(f\"Datos guardados como CSV en HDFS en la ruta: {hdfs_path_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee88a9c",
   "metadata": {},
   "source": [
    "# Análisis de datos procesados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb68d38",
   "metadata": {},
   "source": [
    "## Leyendo datos limpios desde HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec9381d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lugar_viaje: string (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- medio_transporte: string (nullable = true)\n",
      " |-- duracion: string (nullable = true)\n",
      " |-- alojamiento: string (nullable = true)\n",
      "\n",
      "Filas: 100\n",
      "+-----------+----------+----------------+----------------+-----------+\n",
      "|lugar_viaje|     fecha|medio_transporte|        duracion|alojamiento|\n",
      "+-----------+----------+----------------+----------------+-----------+\n",
      "|      Cusco|2025-05-22|           aéreo|   3 días/noches|      hotel|\n",
      "|    Iquitos|2025-08-10|           aéreo|        1 semana|      hotel|\n",
      "|       Lima|2025-02-15|       terrestre|   5 días/noches|       otro|\n",
      "|      Cusco|2025-09-05|            tren|   4 días/noches|       NULL|\n",
      "|       Lima|2025-03-28|            NULL|   fin de semana|       otro|\n",
      "|   Arequipa|2025-07-19|           aéreo|   3 días/noches|      hotel|\n",
      "|       Lima|2025-04-12|       terrestre|   2 días/noches|     hostal|\n",
      "|   Arequipa|2025-08-25|       terrestre|   3 días/noches|       NULL|\n",
      "|      Cusco|2025-07-27|           aéreo|        1 semana|       otro|\n",
      "|      Cusco|2025-09-18|            NULL|1 día (Full Day)|       NULL|\n",
      "+-----------+----------+----------------+----------------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "instagram_processed_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(\"hdfs://localhost:9000/user/user/processed/instagram/instagram_posts_clean.csv\")\n",
    "\n",
    "instagram_processed_df.printSchema()\n",
    "print(\"Filas:\", instagram_processed_df.count())\n",
    "instagram_processed_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cfea3b",
   "metadata": {},
   "source": [
    "# Deteniendo sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "942c0f88-56b2-4734-8077-8398bd1f6ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado. Sesión de Spark detenida.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. DETENER LA SESIÓN DE SPARK\n",
    "# ==============================================================================\n",
    "spark.stop()\n",
    "print(\"Proceso completado. Sesión de Spark detenida.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Big Data)",
   "language": "python",
   "name": "bigdata_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
